# -*- coding: utf-8 -*-
"""[model1] Inference_seq2seq.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GAx-h_SK2fVBkzDXogwXcdt1hw-lVb3N

# Khai báo mô hình
"""

import random
import torch
import torchtext
import torch.nn as nn
import pickle

# set the device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
from pyvi import ViTokenizer

'''
tokenization code
'''

import spacy
spacy_vi = spacy.load('vi_core_news_lg')
stopwords = spacy_vi.Defaults.stop_words

def tokenize_vi(text):
    """
    Tokenizes Vietnamese text from a string into a list of strings (tokens)
    """
    return [tok.text for tok in spacy_vi.tokenizer(text)]



class Encoder(nn.Module):
    def __init__(self, input_dim, emb_dim, enc_hid_dim,n_layers, dropout):
        super().__init__()

        self.emb_dim = emb_dim
        self.enc_hid_dim = enc_hid_dim
        self.dropout = dropout
        self.n_layers = n_layers

        # self.embedding = nn.Embedding(input_dim, emb_dim)
        # further you can now make use of your pretrained embeddings inside your model by passing them in
        # as a parameter or loading the handy pickle file mentioned above - a much better design pattern
        # indeed than relying on TEXT.build_vocab() in order to define the embedding layer of your model
        self.embedding = nn.Embedding.from_pretrained(SRC_saved.vocab.vectors, freeze=False)
        # freeze=True <-> self.embedding.requires_grad = False, not learn in training
        # SRC.vocab.vectors, freeze=False

        self.lstm = nn.LSTM(emb_dim, enc_hid_dim, n_layers, dropout=dropout)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, src):
        
        #src = [src len, batch size]
        
        embedded = self.dropout(self.embedding(src))
        
        #embedded = [src len, batch size, emb dim]
        
        outputs, (hidden, cell) = self.lstm(embedded)
        
        return hidden, cell

class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, dec_hid_dim, n_layers, dropout):
        super().__init__()

        self.emb_dim = emb_dim
        self.output_dim = output_dim
        self.dec_hid_dim = dec_hid_dim
        self.n_layers = n_layers
        self.dropout = dropout

        # self.embedding = nn.Embedding(output_dim, emb_dim)
        self.embedding = nn.Embedding.from_pretrained(TRG_saved.vocab.vectors, freeze=False)
        # self.embedding.requires_grad = False
        self.lstm = nn.LSTM(emb_dim, dec_hid_dim, n_layers, dropout=dropout)
        self.fc_out = nn.Linear(dec_hid_dim, output_dim)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, input, hidden, cell):
        
        input = input.unsqueeze(0)
        
        embedded = self.dropout(self.embedding(input))
        
        #embedded = [1, batch size, emb dim]    
        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))
        
        # predicted shape is [batch_size, output_dim]
        prediction = self.fc_out(hidden.squeeze(0))
        
        return prediction, hidden, cell

class Seq2Seq(nn.Module):
    ''' 
    Args:
        encoder: A Encoder class instance.
        decoder: A Decoder class instance.
    '''
    def __init__(self, encoder, decoder, device):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device

    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        batch_size = trg.shape[1]
        max_len = trg.shape[0]
        trg_vocab_size = self.decoder.output_dim

        # to store the outputs of the decoder
        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)

        hidden, cell = self.encoder(src)

        # first input to the decoder is the <sos> tokens
        input = trg[0, :]

        for t in range(1, max_len):
            output, hidden, cell = self.decoder(input, hidden, cell)
            outputs[t] = output
            use_teacher_force = random.random() < teacher_forcing_ratio
            top1 = output.max(1)[1]
            input = (trg[t] if use_teacher_force else top1)

        # outputs is of shape [sequence_len, batch_size, output_dim]
        return outputs

'''
load fields saved during preprocessing
'''
with open("TRG.Field","rb") as f:
     TRG_saved = pickle.load(f)

with open("SRC.Field","rb") as f:
     SRC_saved = pickle.load(f)

'''
hyperparameters (ensure the following hyperparameters match with those used during training of the best model)
'''
INPUT_DIM = len(SRC_saved.vocab)
OUTPUT_DIM = len(TRG_saved.vocab)
ENC_EMB_DIM = 300
DEC_EMB_DIM = 300
ENC_HID_DIM = 512
DEC_HID_DIM = 512
ENC_DROPOUT = 0.5
DEC_DROPOUT = 0.5
N_LAYERS = 1

'''
instantiate the model
'''
enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT)
dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT)
model_best = Seq2Seq(enc, dec, device).to(device)

'''
load the checkpoint corresponding to the best epoch (usually epoch with highest validation BLEU score)
'''
# n_ckpt = 200
n_ckpt = 237
model_best.load_state_dict(torch.load(f'ckpt_seq2seq/seq2seq_{n_ckpt}.pt')['state_dict'])
model_best = model_best.to(device)

test_data = torchtext.legacy.data.TabularDataset(
    path="test_paraphrase.csv", 
    format='csv', skip_header=True, fields=[('src', SRC_saved)])

test_iter = torchtext.legacy.data.Iterator(test_data, batch_size=256, device=device, sort=False, sort_key=None, shuffle=False, sort_within_batch=False)

# convert index to text string
def convert_itos(convert_vocab, token_ids):
    list_string = []
    for i in token_ids:
        if i == convert_vocab.vocab.stoi['<eos>']:
            break
        else:
            token = convert_vocab.vocab.itos[i]
            list_string.append(token)
    return list_string

TRG_PAD_IDX = TRG_saved.vocab.stoi[TRG_saved.pad_token]
print(TRG_PAD_IDX)

'''
generate paraphrases for all the sentences in test data
'''
def generate_paraphrases(model, eval_iter, trg_vocab, attention = True, max_trg_len = 64):
    model.eval()
    all_translation_word_ids = []
    for batch in test_iter:
        src = batch.src
        batch_size = src.shape[1]


        trg_placeholder = torch.Tensor(max_trg_len, batch_size)
        trg_placeholder.fill_(TRG_PAD_IDX)
        trg_placeholder = trg_placeholder.long().to(device)
        if attention == True:
          output,_ = model(src, trg_placeholder, 0) #turn off teacher forcing
        else:
          output = model(src, trg_placeholder, 0) #turn off teacher forcing
        
        output_translate = output[1:]

        # Choose top 1 word from decoder's output, we get the probability and index of the word
        prob, token_id = output_translate.data.topk(1)
        translation_token_id = token_id.squeeze(2).cpu()

        # store gold target sentences to a list 
        all_translation_word_ids.append(translation_token_id)

    all_translation_text = []
    for i in range(len(all_translation_word_ids)):
      cur_translation_batch = all_translation_word_ids[i]
      for j in range(cur_translation_batch.shape[1]):
        trans_convered_strings = convert_itos(trg_vocab, cur_translation_batch[:,j])
        all_translation_text.append(' '.join(trans_convered_strings)) # convert list of words to text
    
    return all_translation_text

# Output đầu ra là các câu đồng nghĩa
# output all the sentences in the test set    
test_predictions = generate_paraphrases(model_best, test_iter, TRG_saved, attention = False, max_trg_len = 64)

import pandas as pd
test_csv = pd.read_csv("test_paraphrase.csv")["src"].tolist()
test_csv

for (orig_sent, pred_sent) in zip(test_csv, test_predictions):
  print("Orig: ", orig_sent)
  print("Pred: ", pred_sent)

test_predictions

